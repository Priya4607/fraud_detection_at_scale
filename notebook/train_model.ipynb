{"cells":[{"cell_type":"markdown","source":["This notebook trains a fraud detection model using simulated credit card transactions history generated for 6 months. Usually training a model to detect fraudulent transaction is quite difficult considering the low nnumber of confirmed instances of fraudulent behaviour."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DETECTING FRADULENT TRANSACTIONS AT SCALE","showTitle":true,"inputWidgets":{},"nuid":"b04f9fad-3935-4ef1-b7e6-42d3348da247"}}},{"cell_type":"code","source":["import os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nfrom pyspark.sql import DataFrame, SparkSession\nfrom pyspark.sql.functions import dense_rank, desc\nfrom pyspark.sql.types import (\n    DoubleType,\n    LongType,\n    StringType,\n    StructField,\n    StructType,\n    TimestampType,\n    IntegerType,\n)\nfrom pyspark.sql.window import Window\nfrom sklearn.model_selection import train_test_split\nfrom pyspark.sql.functions import round"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43ee56b2-ef76-4c14-be40-c4337966fdcd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### DATA AGGREGATION\nThe following block implements a couple of window function to gather weekly/10 min aggregation of average amount/number of transactions per credit card. The average amount and number of transactions are normlalized by dividing the same over the weekly aggregated amount and number of transactions. \n\nIn addition to the above, the aggregated features are grouped by credit card number and selected features are written to a file (exploring the possibility of using online feature store like Feast to record to store selected features. More info on this can be found at the experimental section of the repository)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5ece4d3-2927-4d94-9a1b-ae0042290bad"}}},{"cell_type":"code","source":["def aggregate_features(spark):\n    schema = StructType([StructField('txn_id', StringType(), True),\n                         StructField('cc_num', LongType(), True),\n                         StructField('ts', TimestampType(), True),\n                         StructField('amt', DoubleType(), True),\n                         StructField('label', DoubleType(), True)])\n    transactions_df = spark.read.csv('dbfs:/FileStore/tables/transactions.csv', \\\n                                     header=False, \\\n                                     schema=schema)\n    query = \"\"\"\n    SELECT *, \\\n           avg_amt_last_10m/avg_amt_last_1w AS amt_ratio1, \\\n           amt/avg_amt_last_1w AS amt_ratio2, \\\n           num_trans_last_10m/num_trans_last_1w AS count_ratio \\\n    FROM \\\n        ( \\\n        SELECT *, \\\n               COUNT(*) OVER w1 as num_trans_last_10m, \\\n               AVG(amt) OVER w1 as avg_amt_last_10m, \\\n               COUNT(*) OVER w2 as num_trans_last_1w, \\\n               AVG(amt) OVER w2 as avg_amt_last_1w \\\n        FROM transactions_df \\\n        WINDOW \\\n               w1 AS (PARTITION BY cc_num order by cast(ts AS timestamp) RANGE INTERVAL 10 MINUTE PRECEDING), \\\n               w2 AS (PARTITION BY cc_num order by cast(ts AS timestamp) RANGE INTERVAL 1 WEEK PRECEDING) \\\n        ) \n    \"\"\"\n    transactions_df.createOrReplaceTempView('transactions_df')\n    aggregated_features = spark.sql(query)\n    return aggregated_features\n\ndef group_by_cc(aggregated_features):\n    window = Window.partitionBy('cc_num').orderBy(desc('ts'))\n    sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n    grouped_df = sorted_df.filter(sorted_df.rank == 1).drop(sorted_df.rank)\n    sliced_df = grouped_df.select('ts', 'cc_num','num_trans_last_10m','avg_amt_last_10m','num_trans_last_1w','avg_amt_last_1w')\n    sliced_df = sliced_df.withColumn('avg_amt_last_1w', round('avg_amt_last_1w', 2))\n    return sliced_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82e96505-949b-46c8-bd1d-b02fb2f84291"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.rm('/FileStore/tables/aggregated_output.csv', True)\ndbutils.fs.rm('/FileStore/tables/aggregated_output.parquet', True)\nspark = SparkSession.builder.appName('SparkFraudAnalysis').getOrCreate()\ndf = aggregate_features(spark)\nsliced_df = group_by_cc(df)\nsliced_df.coalesce(1).write.csv('dbfs:/FileStore/tables/aggregated_output.csv')\nsliced_df.coalesce(1).write.parquet('dbfs:/FileStore/tables/aggregated_output.parquet')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d3f1127-6be0-47cb-969a-07d12db825f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Create a set of training and testing data (last couple of weeks used for this)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c683abe-0224-4229-a05a-c22104a6125e"}}},{"cell_type":"code","source":["dbutils.fs.rm('/FileStore/tables/transactions_test.csv', True)\nstart = '2021-08-15'\nend = '2022-01-15'\ntrain_df = df.filter( (df.ts  > start) & (df.ts  < end) )\ntest_df = df.filter((df.ts > end))\ntest_df.coalesce(1).write.csv('dbfs:/FileStore/tables/transactions_test.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c61490ad-3dad-4250-bfb6-b09934f867b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import round\ntrain_df = train_df.select('label', 'amt', 'amt_ratio1','amt_ratio2','count_ratio')\n\nfor col in train_df.columns:\n    if col != 'label':\n      train_df = train_df.withColumn(col, round(col, 2))\n      \ntest_df = test_df.select('label', 'amt', 'amt_ratio1', 'amt_ratio2', 'count_ratio')\nfor col in test_df.columns:\n    if col != 'label':\n      test_df = test_df.withColumn(col, round(col, 2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa3a6c85-317c-46cd-a63b-43d073adb480"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### TRAINING MODEL\nOnce the raw dataset is transformed to extract spending patterns, we use the normalized ratios as features to train the model using XGBoost. We evaluate the model performance against the metrics precisely area under ROC and area under Precision-Recall. Additionally, Mlflow is used to track and compare metrics between different runs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20ec9587-cae7-40f8-9c7e-34be68a915ec"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluatorPR = BinaryClassificationEvaluator(labelCol = \"label\", rawPredictionCol = \"prediction\", metricName = \"areaUnderPR\")\nevaluatorAUC = BinaryClassificationEvaluator(labelCol = \"label\", rawPredictionCol = \"prediction\", metricName = \"areaUnderROC\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30e498a0-87cc-4ded-bd43-f192c9aa8fc4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom sparkdl.xgboost import XgboostClassifier\nfrom pyspark.ml import Pipeline\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nfeaturesCols = train_df.columns\nfeaturesCols.remove('label')\n\nvectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"features\")\n\nxgb = XgboostClassifier(num_workers=3, labelCol='label', missing=0.0)\nevaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\ncv = CrossValidator(estimator=Pipeline(stages = [vectorAssembler, xgb]),\n            estimatorParamMaps=ParamGridBuilder() \\\n                                .addGrid(xgb.max_depth, [2, 5])\\\n                                .addGrid(xgb.n_estimators, [10, 100])\\\n                                .build(),                             \n            evaluator=evaluator,\n            numFolds=3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a7548f3-56f3-4c53-b2d4-73992bac6d55"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import mlflow\nimport mlflow.spark\nwith mlflow.start_run():\n  cvModel = cv.fit(train_df)\n  test_metric_PR = evaluatorPR.evaluate(cvModel.transform(test_df))\n  test_metric_AUC = evaluatorAUC.evaluate(cvModel.transform(test_df))\n  train_metric_PR = evaluatorPR.evaluate(cvModel.transform(train_df))\n  train_metric_AUC  = evaluatorAUC.evaluate(cvModel.transform(train_df))\n  mlflow.log_metric('test_PR_' + evaluatorPR.getMetricName(), test_metric_PR) \n  mlflow.log_metric('test_AUC_' + evaluatorAUC.getMetricName(), test_metric_AUC) \n  mlflow.log_metric('train_PR_' + evaluatorPR.getMetricName(), train_metric_PR) \n  mlflow.log_metric('train_AUC_' + evaluatorAUC.getMetricName(), train_metric_AUC)\n  mlflow.spark.log_model(spark_model=cvModel.bestModel, artifact_path='best_model_xgb') "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d4e323e-4cd2-42d3-8e42-f03593a36bb4"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"train_model","dashboards":[{"elements":[{"elementNUID":"b04f9fad-3935-4ef1-b7e6-42d3348da247","guid":"92714b6f-a933-47d7-aec7-50a25ba3dd2a","resultIndex":null,"options":null,"position":{"x":0,"y":0,"height":3,"width":12,"z":null},"elementType":"command"}],"guid":"44e66357-89d7-4684-bb9c-ffc45cae0d45","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"e2cef7dc-bb20-45fd-a61e-efc71bc57bc3","origId":3943504546824768,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3071962839376406}},"nbformat":4,"nbformat_minor":0}
